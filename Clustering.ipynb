{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "from scipy.stats import kruskal\n",
    "from scipy.stats import chi2_contingency\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from scipy import linalg\n",
    "from scipy.sparse.csgraph import laplacian\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import spectral_embedding\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_settings = {\"ml\": {\"model\": {\"n_clusters\": 2}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(adjacency):\n",
    "        \"\"\"Clusters the sequences based on their adjacency matrix\n",
    "\n",
    "        Args:\n",
    "            adjacency: adjacency matrix of the sequences to cluster\n",
    "            n_clusters: number of clusters\n",
    "            normed: normalised or not for the laplacian\n",
    "    \n",
    "        Returns:\n",
    "            sklearn clustering object\n",
    "            projection of the data points\n",
    "            eigen values\n",
    "\n",
    "        \"\"\"\n",
    "        L = laplacian(adjacency, normed=True)\n",
    "        eigenvals, _ = linalg.eig(L)\n",
    "        eigenvals = np.real(eigenvals)\n",
    "        eigenvals_sorted = eigenvals[np.argsort(eigenvals)]\n",
    "\n",
    "        # Create embedding\n",
    "        random_state = np.random.RandomState(193)\n",
    "        proj_X = spectral_embedding(adjacency, n_components=model_settings['ml']['model']['n_clusters'],\n",
    "                                random_state=random_state,\n",
    "                                drop_first=False)\n",
    "\n",
    "        # Cluster the points using k-means clustering\n",
    "        kmeans = KMeans(model_settings['ml']['model']['n_clusters'], random_state = random_state, n_init=10)\n",
    "        kmeans.fit(proj_X)\n",
    "        labels = kmeans.labels_\n",
    "\n",
    "        details = {\n",
    "            'model': kmeans,\n",
    "            'projection': proj_X,\n",
    "            'eigenvalues': eigenvals_sorted\n",
    "        }\n",
    "\n",
    "        return labels, details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sequences):\n",
    "    \"\"\"Trains the algorithms for different values of k, then returns the best one\n",
    "\n",
    "    Args:\n",
    "        sequences (_type_): already formatted sequences\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'sequences': sequences,\n",
    "        'lids': sorted_session_codes, # A list of students unique ids\n",
    "        'k-range': [\n",
    "            2,\n",
    "            20\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    for k in range(results['k-range'][0], results['k-range'][1]):\n",
    "        model_settings['ml']['model']['n_clusters'] = k\n",
    "        labels, details = cluster(sequences)\n",
    "\n",
    "        scores = silhouette_score(sequences, labels)\n",
    "\n",
    "        results[k] = {\n",
    "            'labels': labels,\n",
    "            'details': details,\n",
    "            'scores': scores\n",
    "        }\n",
    "        \n",
    "        print('    scores for {}: {}'.format(k, scores))\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fin = pd.read_excel('///.xlsx') # A table containing students post-test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, L, k, x0):\n",
    "    return 1 + (L - 1) / (1 + np.exp(-k * (x - x0)))\n",
    "\n",
    "# Parameters\n",
    "L = 2  # Maximum value (factor should approach 2)\n",
    "k = 1  # Adjust the steepness of the curve\n",
    "x0 = 3  # Midpoint where the rate of increase slows down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_two_histograms(data1, data2, xlabel, bins, range_hist):\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(6, 5), sharex=True, sharey=True)  # 2 rows, 1 column\n",
    "\n",
    "    weights1 = np.ones_like(data1) / len(data1) * 100\n",
    "    weights2 = np.ones_like(data2) / len(data2)* 100\n",
    "\n",
    "    axs[0].hist(data1, bins=bins, range=range_hist, weights=weights1, color='green', alpha=0.7, edgecolor='black', linewidth=1)\n",
    "    axs[0].set_title('Cluster 1')\n",
    "    axs[0].set_ylabel('Percentage of students')\n",
    "\n",
    "    axs[1].hist(data2, bins=bins, range=range_hist, weights=weights2, color='orange', alpha=0.7, edgecolor='black', linewidth=1)\n",
    "    axs[1].set_title('Cluster 2')\n",
    "    axs[1].set_xlabel(xlabel)\n",
    "    axs[1].set_ylabel('Percentage of students')\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout for better spacing\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_three_histograms(data1, data2, data3, xlabel, bins, range_hist):\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(6, 5), sharex=True, sharey=True)  # 3 rows, 1 column\n",
    "\n",
    "    weights1 = np.ones_like(data1) / len(data1) * 100\n",
    "    weights2 = np.ones_like(data2) / len(data2)* 100\n",
    "    weights3 = np.ones_like(data3) / len(data3)* 100\n",
    "\n",
    "    axs[0].hist(data1, bins=bins, range=range_hist, weights=weights1, color='green', alpha=0.7, edgecolor='black', linewidth=1)\n",
    "    axs[0].set_title('Cluster 1')\n",
    "\n",
    "    axs[1].hist(data2, bins=bins, range=range_hist, weights=weights2, color='orange', alpha=0.7, edgecolor='black', linewidth=1)\n",
    "    axs[1].set_title('Cluster 2')\n",
    "    axs[1].set_ylabel('Percentage of students')\n",
    "\n",
    "    axs[2].hist(data3, bins=bins, range=range_hist, weights=weights3, color='blue', alpha=0.7, edgecolor='black', linewidth=1)\n",
    "    axs[2].set_title('Cluster 3')\n",
    "    axs[2].set_xlabel(xlabel)\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout for better spacing\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_four_histograms(data1, data2, data3, data4, xlabel, bins, range_hist):\n",
    "    fig, axs = plt.subplots(4, 1, figsize=(8, 7), sharex=True, sharey=True)  # 4 rows, 1 column\n",
    "\n",
    "    weights1 = np.ones_like(data1) / len(data1) * 100\n",
    "    weights2 = np.ones_like(data2) / len(data2)* 100\n",
    "    weights3 = np.ones_like(data3) / len(data3)* 100\n",
    "    weights4 = np.ones_like(data4) / len(data4)* 100\n",
    "\n",
    "    axs[0].hist(data1, bins=bins, range=range_hist, weights=weights1, color='green', alpha=0.7, edgecolor='black', linewidth=1)\n",
    "    axs[0].set_title('Cluster 1')\n",
    "\n",
    "    axs[1].hist(data2, bins=bins, range=range_hist, weights=weights2, color='orange', alpha=0.7, edgecolor='black', linewidth=1)\n",
    "    axs[1].set_title('Cluster 2')\n",
    "    axs[1].set_ylabel('Percentage of students')\n",
    "\n",
    "    axs[2].hist(data3, bins=bins, range=range_hist, weights=weights3, color='blue', alpha=0.7, edgecolor='black', linewidth=1)\n",
    "    axs[2].set_title('Cluster 3')\n",
    "\n",
    "    axs[3].hist(data4, bins=bins, range=range_hist, weights=weights4, color='red', alpha=0.7, edgecolor='black', linewidth=1)\n",
    "    axs[3].set_title('Cluster 4')\n",
    "    axs[3].set_xlabel(xlabel)\n",
    "    axs[3].set_ylabel('Percentage of students')\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout for better spacing\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General functions for clusters analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vector(original_vector, sorted_session_codes, cluster_codes): # cluster_codes is a list containing student ids in a particular cluster\n",
    "    vector = []\n",
    "\n",
    "    for i in range(len(original_vector)):\n",
    "        if sorted_session_codes[i] in cluster_codes:\n",
    "            vector.append(original_vector[i])\n",
    "\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_three_instructions(new_labels, instructions, sorted_session_codes, cluster_codes):\n",
    "    count_A_DC = 0\n",
    "    count_A_DI = 0\n",
    "    count_B = 0\n",
    "\n",
    "    for i in range(len(new_labels)):\n",
    "        if sorted_session_codes[i] in cluster_codes:\n",
    "            if 'A_DC' in instructions[i]:\n",
    "                count_A_DC += 1\n",
    "            elif 'A_DI' in instructions[i]:\n",
    "                count_A_DI += 1\n",
    "            else:\n",
    "                count_B += 1\n",
    "\n",
    "    return count_A_DC, count_A_DI, count_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_prior_knowledge(new_labels, df_fin, sorted_session_codes, cluster_codes):\n",
    "    count_ja = 0\n",
    "    count_nein = 0\n",
    "\n",
    "    for i in range(len(new_labels)):\n",
    "        if sorted_session_codes[i] in cluster_codes:\n",
    "            if df_fin['bll_prior'][i] == 'Ja':\n",
    "                count_ja += 1\n",
    "            elif df_fin['bll_prior'][i] == 'Nein':\n",
    "                count_nein += 1\n",
    "\n",
    "    return count_ja, count_nein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bll_cluster(df_fin, cluster_codes):\n",
    "    bll = []\n",
    "\n",
    "    for i in range(len(df_fin['session_code_sim1'])):\n",
    "        if df_fin['session_code_sim1'][i] in cluster_codes and math.isnan(df_fin['bll_weighed (12)'][i]) == False:\n",
    "            bll.append(df_fin['bll_weighed (12)'][i])\n",
    "\n",
    "    return bll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_asterisks_for_pval(p_val):\n",
    "    \"\"\"Receives the p-value and returns asterisks string.\"\"\"\n",
    "    if p_val > 0.05:\n",
    "        p_text = \"ns\"  # above threshold => not significant\n",
    "    elif p_val < 1e-4:  \n",
    "        p_text = '****'\n",
    "    elif p_val < 1e-3:\n",
    "        p_text = '***'\n",
    "    elif p_val < 1e-2:\n",
    "        p_text = '**'\n",
    "    else:\n",
    "        p_text = '*'\n",
    "    \n",
    "    return p_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chisq_and_posthoc_corrected(df):\n",
    "    \"\"\"Receives a dataframe and performs chi2 test and then post hoc.\n",
    "    Prints the p-values and corrected p-values (after FDR correction)\"\"\"\n",
    "    # start by running chi2 test on the matrix\n",
    "    chi2, p, dof, ex = chi2_contingency(df, correction=True)\n",
    "    print(f\"Chi2 result of the contingency table: {chi2}, p-value: {p}\")\n",
    "    \n",
    "    # post-hoc\n",
    "    all_combinations = list(combinations(df.index, 2))  # gathering all combinations for post-hoc chi2\n",
    "    p_vals = []\n",
    "    print(\"Significance results:\")\n",
    "    for comb in all_combinations:\n",
    "        new_df = df[(df.index == comb[0]) | (df.index == comb[1])]\n",
    "        chi2, p, dof, ex = chi2_contingency(new_df, correction=True)\n",
    "        p_vals.append(p)\n",
    "        # print(f\"For {comb}: {p}\")  # uncorrected\n",
    "\n",
    "    # checking significance\n",
    "    # correction for multiple testing using BH\n",
    "    reject_list, corrected_p_vals, _, _ = multipletests(p_vals, method='fdr_bh')\n",
    "    for p_val, corr_p_val, reject, comb in zip(p_vals, corrected_p_vals, reject_list, all_combinations):\n",
    "        print(f\"{comb}: p_value: {p_val:5f}; corrected: {corr_p_val:5f} ({get_asterisks_for_pval(p_val)}) reject: {reject}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions 2 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_cluster_codes(sorted_session_codes, labels_2):\n",
    "    cluster_1_codes = []\n",
    "    cluster_2_codes = []\n",
    "\n",
    "    for i in range(len(sorted_session_codes)):\n",
    "        if labels_2[i] == 0:\n",
    "            cluster_1_codes.append(sorted_session_codes[i])\n",
    "        else:\n",
    "            cluster_2_codes.append(sorted_session_codes[i])\n",
    "\n",
    "    return cluster_1_codes, cluster_2_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_clusters_one_vector(new_labels, df_fin, instructions, sorted_session_codes, labels_2, original_vector1):\n",
    "\n",
    "    original_vector1_name = input(\"Please enter an original_vector1 name: \")\n",
    "\n",
    "    cluster_1_codes, cluster_2_codes = two_cluster_codes(sorted_session_codes, labels_2)\n",
    "    print(\"Cluster 1:\", len(cluster_1_codes), \"students,\", \"Cluster 2:\", len(cluster_2_codes), \"students\")\n",
    "\n",
    "\n",
    "    vector_1 = build_vector(original_vector1, sorted_session_codes, cluster_1_codes)\n",
    "    vector_2 = build_vector(original_vector1, sorted_session_codes, cluster_2_codes)\n",
    "    plot_two_histograms(vector_1, vector_2, f'Percentage of {original_vector1_name} actions', bins=10, range_hist=(0, 1))\n",
    "    print(kruskal(vector_1, vector_2))\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "    count_A_DC_1, count_A_DI_1, count_B_1 = count_three_instructions(new_labels, instructions, sorted_session_codes, cluster_1_codes)\n",
    "    count_A_DC_2, count_A_DI_2, count_B_2 = count_three_instructions(new_labels, instructions, sorted_session_codes, cluster_2_codes)\n",
    "\n",
    "    data = np.array([[count_A_DC_1, count_A_DC_2], [count_A_DI_1, count_A_DI_2], [count_B_1, count_B_2]])\n",
    "    chi2, p, _, _ = chi2_contingency(data)\n",
    "    print(f'A_DC_1: {count_A_DC_1},', f'A_DC_2: {count_A_DC_2}')\n",
    "    print(f'A_DI_1: {count_A_DI_1},', f'A_DI_2: {count_A_DI_2}')\n",
    "    print(f'B_1: {count_B_1},', f'B_2: {count_B_2}')\n",
    "    print(f\"Chi-square statistic: {chi2},\", f\"P-value: {p}\")\n",
    "    print(\"\")\n",
    "\n",
    "    data = np.array([[count_A_DC_1 + count_A_DI_1, count_B_1], [count_A_DC_2 + count_A_DI_2, count_B_2]])\n",
    "    chi2, p, _, _ = chi2_contingency(data)\n",
    "    print(f'A_1: {count_A_DC_1 + count_A_DI_1},', f'A_2: {count_A_DC_2 + count_A_DI_2}')\n",
    "    print(f'B_1: {count_B_1},', f'B_2: {count_B_2}')\n",
    "    print(f\"Chi-square statistic: {chi2},\", f\"P-value: {p}\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "    count_ja_1, count_nein_1 = count_prior_knowledge(new_labels, df_fin, sorted_session_codes, cluster_1_codes)\n",
    "    count_ja_2, count_nein_2 = count_prior_knowledge(new_labels, df_fin, sorted_session_codes, cluster_2_codes)\n",
    "\n",
    "    data = np.array([[count_ja_1, count_ja_2], [count_nein_1, count_nein_2]])\n",
    "    chi2, p, _, _ = chi2_contingency(data)\n",
    "    print(f'Ja_1: {count_ja_1},', f'Ja_2: {count_ja_2}')\n",
    "    print(f'Nein_1: {count_nein_1},', f'Nein_2: {count_nein_2}')\n",
    "    print(f\"Chi-square statistic: {chi2},\", f\"P-value: {p}\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "    bll_1 = bll_cluster(df_fin, cluster_1_codes)\n",
    "    bll_2 = bll_cluster(df_fin, cluster_2_codes)\n",
    "    print(kruskal(bll_1, bll_2))\n",
    "    print(np.mean(bll_1), \"+-\", np.std(bll_1))\n",
    "    print(np.mean(bll_2), \"+-\", np.std(bll_2))\n",
    "    print(\"bll_1:\", bll_1)\n",
    "    print(\"bll_2:\", bll_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-modes functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_modes_count(vector):\n",
    "    my_list_of_tuples = [tuple(sublist) for sublist in vector]\n",
    "    sublist_counts = Counter(my_list_of_tuples)\n",
    "\n",
    "    return sublist_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-modes 2 clusters function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_modes_two_clusters(sorted_session_codes, labels_2, original_vector, new_labels, df_fin, instructions):\n",
    "    cluster_1_codes, cluster_2_codes = two_cluster_codes(sorted_session_codes, labels_2)\n",
    "    print(\"Cluster 1:\", len(cluster_1_codes), \"students,\", \"Cluster 2:\", len(cluster_2_codes), \"students\")\n",
    "    print(\"\")\n",
    "\n",
    "    vector_1 = build_vector(original_vector, sorted_session_codes, cluster_1_codes)\n",
    "    vector_2 = build_vector(original_vector, sorted_session_codes, cluster_2_codes)\n",
    "\n",
    "    sublist_counts_1 = k_modes_count(vector_1)\n",
    "    print(\"Cluster 1:\")\n",
    "    for sublist, count in sublist_counts_1.items():\n",
    "        print(f\"{sublist}: {count} times\")\n",
    "    sublist_counts_2 = k_modes_count(vector_2)\n",
    "    print(\"Cluster 2:\")\n",
    "    for sublist, count in sublist_counts_2.items():\n",
    "        print(f\"{sublist}: {count} times\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "    count_A_DC_1, count_A_DI_1, count_B_1 = count_three_instructions(new_labels, instructions, sorted_session_codes, cluster_1_codes)\n",
    "    count_A_DC_2, count_A_DI_2, count_B_2 = count_three_instructions(new_labels, instructions, sorted_session_codes, cluster_2_codes)\n",
    "\n",
    "    data = np.array([[count_A_DC_1, count_A_DC_2], [count_A_DI_1, count_A_DI_2], [count_B_1, count_B_2]])\n",
    "    chi2, p, _, _ = chi2_contingency(data)\n",
    "    print(f'A_DC_1: {count_A_DC_1},', f'A_DC_2: {count_A_DC_2}')\n",
    "    print(f'A_DI_1: {count_A_DI_1},', f'A_DI_2: {count_A_DI_2}')\n",
    "    print(f'B_1: {count_B_1},', f'B_2: {count_B_2}')\n",
    "    print(f\"Chi-square statistic: {chi2},\", f\"P-value: {p}\")\n",
    "    print(\"\")\n",
    "\n",
    "    data = np.array([[count_A_DC_1 + count_A_DI_1, count_B_1], [count_A_DC_2 + count_A_DI_2, count_B_2]])\n",
    "    chi2, p, _, _ = chi2_contingency(data)\n",
    "    print(f'A_1: {count_A_DC_1 + count_A_DI_1},', f'A_2: {count_A_DC_2 + count_A_DI_2}')\n",
    "    print(f'B_1: {count_B_1},', f'B_2: {count_B_2}')\n",
    "    print(f\"Chi-square statistic: {chi2},\", f\"P-value: {p}\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "    count_ja_1, count_nein_1 = count_prior_knowledge(new_labels, df_fin, sorted_session_codes, cluster_1_codes)\n",
    "    count_ja_2, count_nein_2 = count_prior_knowledge(new_labels, df_fin, sorted_session_codes, cluster_2_codes)\n",
    "\n",
    "    data = np.array([[count_ja_1, count_ja_2], [count_nein_1, count_nein_2]])\n",
    "    chi2, p, _, _ = chi2_contingency(data)\n",
    "    print(f'Ja_1: {count_ja_1},', f'Ja_2: {count_ja_2}')\n",
    "    print(f'Nein_1: {count_nein_1},', f'Nein_2: {count_nein_2}')\n",
    "    print(f\"Chi-square statistic: {chi2},\", f\"P-value: {p}\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "    bll_1 = bll_cluster(df_fin, cluster_1_codes)\n",
    "    bll_2 = bll_cluster(df_fin, cluster_2_codes)\n",
    "    print(kruskal(bll_1, bll_2))\n",
    "    print(\"bll_1:\", bll_1)\n",
    "    print(\"bll_2:\", bll_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-modes 3 clusters function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_modes_three_clusters(sorted_session_codes, labels_3, original_vector, new_labels, df_fin, instructions):\n",
    "    cluster_1_codes, cluster_2_codes, cluster_3_codes = three_cluster_codes(sorted_session_codes, labels_3)\n",
    "    print(\"Cluster 1:\", len(cluster_1_codes), \"students,\", \"Cluster 2:\", len(cluster_2_codes), \"students,\", \"Cluster 3:\", len(cluster_3_codes), \"students\")\n",
    "    print(\"\")\n",
    "\n",
    "    vector_1 = build_vector(original_vector, sorted_session_codes, cluster_1_codes)\n",
    "    vector_2 = build_vector(original_vector, sorted_session_codes, cluster_2_codes)\n",
    "    vector_3 = build_vector(original_vector, sorted_session_codes, cluster_3_codes)\n",
    "\n",
    "    sublist_counts_1 = k_modes_count(vector_1)\n",
    "    print(\"Cluster 1:\")\n",
    "    for sublist, count in sublist_counts_1.items():\n",
    "        print(f\"{sublist}: {count} times\")\n",
    "    sublist_counts_2 = k_modes_count(vector_2)\n",
    "    print(\"Cluster 2:\")\n",
    "    for sublist, count in sublist_counts_2.items():\n",
    "        print(f\"{sublist}: {count} times\")\n",
    "    sublist_counts_3 = k_modes_count(vector_3)\n",
    "    print(\"Cluster 3:\")\n",
    "    for sublist, count in sublist_counts_3.items():\n",
    "        print(f\"{sublist}: {count} times\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "    count_A_DC_1, count_A_DI_1, count_B_1 = count_three_instructions(new_labels, instructions, sorted_session_codes, cluster_1_codes)\n",
    "    count_A_DC_2, count_A_DI_2, count_B_2 = count_three_instructions(new_labels, instructions, sorted_session_codes, cluster_2_codes)\n",
    "    count_A_DC_3, count_A_DI_3, count_B_3 = count_three_instructions(new_labels, instructions, sorted_session_codes, cluster_3_codes)\n",
    "\n",
    "    data = np.array([[count_A_DC_1, count_A_DC_2, count_A_DC_3], [count_A_DI_1, count_A_DI_2, count_A_DI_3], [count_B_1, count_B_2, count_B_3]])\n",
    "    chi2, p, _, _ = chi2_contingency(data)\n",
    "    print(f'A_DC_1: {count_A_DC_1},', f'A_DC_2: {count_A_DC_2}', f'A_DC_3: {count_A_DC_3}')\n",
    "    print(f'A_DI_1: {count_A_DI_1},', f'A_DI_2: {count_A_DI_2}', f'A_DI_3: {count_A_DI_3}')\n",
    "    print(f'B_1: {count_B_1},', f'B_2: {count_B_2}', f'B_3: {count_B_3}')\n",
    "    print(f\"Chi-square statistic: {chi2},\", f\"P-value: {p}\")\n",
    "    print(\"\")\n",
    "\n",
    "    data = np.array([[count_A_DC_1 + count_A_DI_1, count_B_1], [count_A_DC_2 + count_A_DI_2, count_B_2], [count_A_DC_3 + count_A_DI_3, count_B_3]])\n",
    "    chi2, p, _, _ = chi2_contingency(data)\n",
    "    print(f'A_1: {count_A_DC_1 + count_A_DI_1},', f'A_2: {count_A_DC_2 + count_A_DI_2}', f'A_3: {count_A_DC_3 + count_A_DI_3}')\n",
    "    print(f'B_1: {count_B_1},', f'B_2: {count_B_2}', f'B_3: {count_B_3}')\n",
    "    print(f\"Chi-square statistic: {chi2},\", f\"P-value: {p}\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "    count_ja_1, count_nein_1 = count_prior_knowledge(new_labels, df_fin, sorted_session_codes, cluster_1_codes)\n",
    "    count_ja_2, count_nein_2 = count_prior_knowledge(new_labels, df_fin, sorted_session_codes, cluster_2_codes)\n",
    "    count_ja_3, count_nein_3 = count_prior_knowledge(new_labels, df_fin, sorted_session_codes, cluster_3_codes)\n",
    "\n",
    "    data = np.array([[count_ja_1, count_nein_1], [count_ja_2, count_nein_2], [count_ja_3, count_nein_3]])\n",
    "    chi2, p, _, _ = chi2_contingency(data)\n",
    "    print(f'Ja_1: {count_ja_1},', f'Ja_2: {count_ja_2}', f'Ja_3: {count_ja_3}')\n",
    "    print(f'Nein_1: {count_nein_1},', f'Nein_2: {count_nein_2}', f'Nein_3: {count_nein_3}')\n",
    "    print(f\"Chi-square statistic: {chi2},\", f\"P-value: {p}\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "    bll_1 = bll_cluster(df_fin, cluster_1_codes)\n",
    "    bll_2 = bll_cluster(df_fin, cluster_2_codes)\n",
    "    bll_3 = bll_cluster(df_fin, cluster_3_codes)\n",
    "    print(kruskal(bll_1, bll_2, bll_3))\n",
    "    print(\"bll_1:\", bll_1)\n",
    "    print(\"bll_2:\", bll_2)\n",
    "    print(\"bll_3:\", bll_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-modes 4 clusters function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_modes_four_clusters(sorted_session_codes, labels_4, original_vector, new_labels, df_fin, instructions):\n",
    "    cluster_1_codes, cluster_2_codes, cluster_3_codes, cluster_4_codes = four_cluster_codes(sorted_session_codes, labels_4)\n",
    "    print(\"Cluster 1:\", len(cluster_1_codes), \"students,\", \"Cluster 2:\", len(cluster_2_codes), \"students,\", \"Cluster 3:\", len(cluster_3_codes), \"students,\", \"Cluster 4:\", len(cluster_4_codes), \"students\")\n",
    "    print(\"\")\n",
    "\n",
    "    vector_1 = build_vector(original_vector, sorted_session_codes, cluster_1_codes)\n",
    "    vector_2 = build_vector(original_vector, sorted_session_codes, cluster_2_codes)\n",
    "    vector_3 = build_vector(original_vector, sorted_session_codes, cluster_3_codes)\n",
    "    vector_4 = build_vector(original_vector, sorted_session_codes, cluster_4_codes)\n",
    "\n",
    "    sublist_counts_1 = k_modes_count(vector_1)\n",
    "    print(\"Cluster 1:\")\n",
    "    for sublist, count in sublist_counts_1.items():\n",
    "        print(f\"{sublist}: {count} times\")\n",
    "    sublist_counts_2 = k_modes_count(vector_2)\n",
    "    print(\"Cluster 2:\")\n",
    "    for sublist, count in sublist_counts_2.items():\n",
    "        print(f\"{sublist}: {count} times\")\n",
    "    sublist_counts_3 = k_modes_count(vector_3)\n",
    "    print(\"Cluster 3:\")\n",
    "    for sublist, count in sublist_counts_3.items():\n",
    "        print(f\"{sublist}: {count} times\")\n",
    "    sublist_counts_4 = k_modes_count(vector_4)\n",
    "    print(\"Cluster 4:\")\n",
    "    for sublist, count in sublist_counts_4.items():\n",
    "        print(f\"{sublist}: {count} times\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "    count_A_DC_1, count_A_DI_1, count_B_1 = count_three_instructions(new_labels, instructions, sorted_session_codes, cluster_1_codes)\n",
    "    count_A_DC_2, count_A_DI_2, count_B_2 = count_three_instructions(new_labels, instructions, sorted_session_codes, cluster_2_codes)\n",
    "    count_A_DC_3, count_A_DI_3, count_B_3 = count_three_instructions(new_labels, instructions, sorted_session_codes, cluster_3_codes)\n",
    "    count_A_DC_4, count_A_DI_4, count_B_4 = count_three_instructions(new_labels, instructions, sorted_session_codes, cluster_4_codes)\n",
    "\n",
    "    data = np.array([[count_A_DC_1, count_A_DC_2, count_A_DC_3, count_A_DC_4], [count_A_DI_1, count_A_DI_2, count_A_DI_3, count_A_DI_4], [count_B_1, count_B_2, count_B_3, count_B_4]])\n",
    "    chi2, p, _, _ = chi2_contingency(data)\n",
    "    print(f'A_DC_1: {count_A_DC_1},', f'A_DC_2: {count_A_DC_2}', f'A_DC_3: {count_A_DC_3}', f'A_DC_4: {count_A_DC_4}')\n",
    "    print(f'A_DI_1: {count_A_DI_1},', f'A_DI_2: {count_A_DI_2}', f'A_DI_3: {count_A_DI_3}', f'A_DI_4: {count_A_DI_4}')\n",
    "    print(f'B_1: {count_B_1},', f'B_2: {count_B_2}', f'B_3: {count_B_3}', f'B_4: {count_B_4}')\n",
    "    print(f\"Chi-square statistic: {chi2},\", f\"P-value: {p}\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "    data = np.array([[count_A_DC_1 + count_A_DI_1, count_B_1], [count_A_DC_2 + count_A_DI_2, count_B_2], [count_A_DC_3 + count_A_DI_3, count_B_3], [count_A_DC_4 + count_A_DI_4, count_B_4]])\n",
    "    chi2, p, _, _ = chi2_contingency(data)\n",
    "    print(f'A_1: {count_A_DC_1 + count_A_DI_1},', f'A_2: {count_A_DC_2 + count_A_DI_2}', f'A_3: {count_A_DC_3 + count_A_DI_3}', f'A_4: {count_A_DC_4 + count_A_DI_4}')\n",
    "    print(f'B_1: {count_B_1},', f'B_2: {count_B_2}', f'B_3: {count_B_3}', f'B_4: {count_B_4}')\n",
    "    print(f\"Chi-square statistic: {chi2},\", f\"P-value: {p}\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "    count_ja_1, count_nein_1 = count_prior_knowledge(new_labels, df_fin, sorted_session_codes, cluster_1_codes)\n",
    "    count_ja_2, count_nein_2 = count_prior_knowledge(new_labels, df_fin, sorted_session_codes, cluster_2_codes)\n",
    "    count_ja_3, count_nein_3 = count_prior_knowledge(new_labels, df_fin, sorted_session_codes, cluster_3_codes)\n",
    "    count_ja_4, count_nein_4 = count_prior_knowledge(new_labels, df_fin, sorted_session_codes, cluster_4_codes)\n",
    "\n",
    "    data = np.array([[count_ja_1, count_nein_1], [count_ja_2, count_nein_2], [count_ja_3, count_nein_3], [count_ja_4, count_nein_4]])\n",
    "    chi2, p, _, _ = chi2_contingency(data)\n",
    "    print(f'Ja_1: {count_ja_1},', f'Ja_2: {count_ja_2}', f'Ja_3: {count_ja_3}', f'Ja_4: {count_ja_4}')\n",
    "    print(f'Nein_1: {count_nein_1},', f'Nein_2: {count_nein_2}', f'Nein_3: {count_nein_3}', f'Nein_4: {count_nein_4}')\n",
    "    print(f\"Chi-square statistic: {chi2},\", f\"P-value: {p}\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "    bll_1 = bll_cluster(df_fin, cluster_1_codes)\n",
    "    bll_2 = bll_cluster(df_fin, cluster_2_codes)\n",
    "    bll_3 = bll_cluster(df_fin, cluster_3_codes)\n",
    "    bll_4 = bll_cluster(df_fin, cluster_4_codes)\n",
    "    print(kruskal(bll_1, bll_2, bll_3, bll_4))\n",
    "    print(\"bll_1:\", bll_1)\n",
    "    print(\"bll_2:\", bll_2)\n",
    "    print(\"bll_3:\", bll_3)\n",
    "    print(\"bll_4:\", bll_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-modes 5 clusters function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_modes_five_clusters(sorted_session_codes, labels_5, original_vector, new_labels, df_fin, instructions):\n",
    "    cluster_1_codes, cluster_2_codes, cluster_3_codes, cluster_4_codes, cluster_5_codes = five_cluster_codes(sorted_session_codes, labels_5)\n",
    "    print(\"Cluster 1:\", len(cluster_1_codes), \"students,\", \"Cluster 2:\", len(cluster_2_codes), \"students,\", \"Cluster 3:\", len(cluster_3_codes), \"students,\", \"Cluster 4:\", len(cluster_4_codes), \"students,\", \"Cluster 5:\", len(cluster_5_codes), \"students\")\n",
    "    print(\"\")\n",
    "\n",
    "    vector_1 = build_vector(original_vector, sorted_session_codes, cluster_1_codes)\n",
    "    vector_2 = build_vector(original_vector, sorted_session_codes, cluster_2_codes)\n",
    "    vector_3 = build_vector(original_vector, sorted_session_codes, cluster_3_codes)\n",
    "    vector_4 = build_vector(original_vector, sorted_session_codes, cluster_4_codes)\n",
    "    vector_5 = build_vector(original_vector, sorted_session_codes, cluster_5_codes)\n",
    "\n",
    "    sublist_counts_1 = k_modes_count(vector_1)\n",
    "    print(\"Cluster 1:\")\n",
    "    for sublist, count in sublist_counts_1.items():\n",
    "        print(f\"{sublist}: {count} times\")\n",
    "    sublist_counts_2 = k_modes_count(vector_2)\n",
    "    print(\"Cluster 2:\")\n",
    "    for sublist, count in sublist_counts_2.items():\n",
    "        print(f\"{sublist}: {count} times\")\n",
    "    sublist_counts_3 = k_modes_count(vector_3)\n",
    "    print(\"Cluster 3:\")\n",
    "    for sublist, count in sublist_counts_3.items():\n",
    "        print(f\"{sublist}: {count} times\")\n",
    "    sublist_counts_4 = k_modes_count(vector_4)\n",
    "    print(\"Cluster 4:\")\n",
    "    for sublist, count in sublist_counts_4.items():\n",
    "        print(f\"{sublist}: {count} times\")\n",
    "    sublist_counts_5 = k_modes_count(vector_5)\n",
    "    print(\"Cluster 5:\")\n",
    "    for sublist, count in sublist_counts_5.items():\n",
    "        print(f\"{sublist}: {count} times\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "    count_A_DC_1, count_A_DI_1, count_B_1 = count_three_instructions(new_labels, instructions, sorted_session_codes, cluster_1_codes)\n",
    "    count_A_DC_2, count_A_DI_2, count_B_2 = count_three_instructions(new_labels, instructions, sorted_session_codes, cluster_2_codes)\n",
    "    count_A_DC_3, count_A_DI_3, count_B_3 = count_three_instructions(new_labels, instructions, sorted_session_codes, cluster_3_codes)\n",
    "    count_A_DC_4, count_A_DI_4, count_B_4 = count_three_instructions(new_labels, instructions, sorted_session_codes, cluster_4_codes)\n",
    "    count_A_DC_5, count_A_DI_5, count_B_5 = count_three_instructions(new_labels, instructions, sorted_session_codes, cluster_5_codes)\n",
    "\n",
    "    data = np.array([[count_A_DC_1, count_A_DC_2, count_A_DC_3, count_A_DC_4, count_A_DC_5], [count_A_DI_1, count_A_DI_2, count_A_DI_3, count_A_DI_4, count_A_DI_5], [count_B_1, count_B_2, count_B_3, count_B_4, count_B_5]])\n",
    "    chi2, p, _, _ = chi2_contingency(data)\n",
    "    print(f'A_DC_1: {count_A_DC_1},', f'A_DC_2: {count_A_DC_2}', f'A_DC_3: {count_A_DC_3}', f'A_DC_4: {count_A_DC_4}', f'A_DC_5: {count_A_DC_5}')\n",
    "    print(f'A_DI_1: {count_A_DI_1},', f'A_DI_2: {count_A_DI_2}', f'A_DI_3: {count_A_DI_3}', f'A_DI_4: {count_A_DI_4}', f'A_DI_5: {count_A_DI_5}')\n",
    "    print(f'B_1: {count_B_1},', f'B_2: {count_B_2}', f'B_3: {count_B_3}', f'B_4: {count_B_4}', f'B_5: {count_B_5}')\n",
    "    print(f\"Chi-square statistic: {chi2},\", f\"P-value: {p}\")\n",
    "    print(\"\")\n",
    "\n",
    "    data = np.array([[count_A_DC_1 + count_A_DI_1, count_B_1], [count_A_DC_2 + count_A_DI_2, count_B_2], [count_A_DC_3 + count_A_DI_3, count_B_3], [count_A_DC_4 + count_A_DI_4, count_B_4], [count_A_DC_5 + count_A_DI_5, count_B_5]])\n",
    "    chi2, p, _, _ = chi2_contingency(data)\n",
    "    print(f'A_1: {count_A_DC_1 + count_A_DI_1},', f'A_2: {count_A_DC_2 + count_A_DI_2}', f'A_3: {count_A_DC_3 + count_A_DI_3}', f'A_4: {count_A_DC_4 + count_A_DI_4}', f'A_5: {count_A_DC_5 + count_A_DI_5}')\n",
    "    print(f'B_1: {count_B_1},', f'B_2: {count_B_2}', f'B_3: {count_B_3}', f'B_4: {count_B_4}', f'B_5: {count_B_5}')\n",
    "    print(f\"Chi-square statistic: {chi2},\", f\"P-value: {p}\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "    count_ja_1, count_nein_1 = count_prior_knowledge(new_labels, df_fin, sorted_session_codes, cluster_1_codes)\n",
    "    count_ja_2, count_nein_2 = count_prior_knowledge(new_labels, df_fin, sorted_session_codes, cluster_2_codes)\n",
    "    count_ja_3, count_nein_3 = count_prior_knowledge(new_labels, df_fin, sorted_session_codes, cluster_3_codes)\n",
    "    count_ja_4, count_nein_4 = count_prior_knowledge(new_labels, df_fin, sorted_session_codes, cluster_4_codes)\n",
    "    count_ja_5, count_nein_5 = count_prior_knowledge(new_labels, df_fin, sorted_session_codes, cluster_5_codes)\n",
    "\n",
    "    data = np.array([[count_ja_1, count_ja_2, count_ja_3, count_ja_4, count_ja_5], [count_nein_1, count_nein_2, count_nein_3, count_nein_4, count_nein_5]])\n",
    "    chi2, p, _, _ = chi2_contingency(data)\n",
    "    print(f'Ja_1: {count_ja_1},', f'Ja_2: {count_ja_2}', f'Ja_3: {count_ja_3}', f'Ja_4: {count_ja_4}', f'Ja_5: {count_ja_5}')\n",
    "    print(f'Nein_1: {count_nein_1},', f'Nein_2: {count_nein_2}', f'Nein_3: {count_nein_3}', f'Nein_4: {count_nein_4}', f'Nein_5: {count_nein_5}')\n",
    "    print(f\"Chi-square statistic: {chi2},\", f\"P-value: {p}\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "    bll_1 = bll_cluster(df_fin, cluster_1_codes)\n",
    "    bll_2 = bll_cluster(df_fin, cluster_2_codes)\n",
    "    bll_3 = bll_cluster(df_fin, cluster_3_codes)\n",
    "    bll_4 = bll_cluster(df_fin, cluster_4_codes)\n",
    "    bll_5 = bll_cluster(df_fin, cluster_5_codes)\n",
    "    print(kruskal(bll_1, bll_2, bll_3, bll_4, bll_5))\n",
    "    print(\"bll_1:\", bll_1)\n",
    "    print(\"bll_2:\", bll_2)\n",
    "    print(\"bll_3:\", bll_3)\n",
    "    print(\"bll_4:\", bll_4)\n",
    "    print(\"bll_5:\", bll_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-modes 6 clusters function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_modes_six_clusters(sorted_session_codes, labels_6, original_vector, new_labels, df_fin, instructions):\n",
    "    cluster_1_codes, cluster_2_codes, cluster_3_codes, cluster_4_codes, cluster_5_codes, cluster_6_codes = six_cluster_codes(sorted_session_codes, labels_6)\n",
    "    print(\"Cluster 1:\", len(cluster_1_codes), \"students,\", \"Cluster 2:\", len(cluster_2_codes), \"students,\", \"Cluster 3:\", len(cluster_3_codes), \"students,\", \"Cluster 4:\", len(cluster_4_codes), \"students,\", \"Cluster 5:\", len(cluster_5_codes), \"students\", \"Cluster 6:\", len(cluster_6_codes), \"students\")\n",
    "    print(\"\")\n",
    "\n",
    "    vector_1 = build_vector(original_vector, sorted_session_codes, cluster_1_codes)\n",
    "    vector_2 = build_vector(original_vector, sorted_session_codes, cluster_2_codes)\n",
    "    vector_3 = build_vector(original_vector, sorted_session_codes, cluster_3_codes)\n",
    "    vector_4 = build_vector(original_vector, sorted_session_codes, cluster_4_codes)\n",
    "    vector_5 = build_vector(original_vector, sorted_session_codes, cluster_5_codes)\n",
    "    vector_6 = build_vector(original_vector, sorted_session_codes, cluster_6_codes)\n",
    "\n",
    "    sublist_counts_1 = k_modes_count(vector_1)\n",
    "    print(\"Cluster 1:\")\n",
    "    for sublist, count in sublist_counts_1.items():\n",
    "        print(f\"{sublist}: {count} times\")\n",
    "    sublist_counts_2 = k_modes_count(vector_2)\n",
    "    print(\"Cluster 2:\")\n",
    "    for sublist, count in sublist_counts_2.items():\n",
    "        print(f\"{sublist}: {count} times\")\n",
    "    sublist_counts_3 = k_modes_count(vector_3)\n",
    "    print(\"Cluster 3:\")\n",
    "    for sublist, count in sublist_counts_3.items():\n",
    "        print(f\"{sublist}: {count} times\")\n",
    "    sublist_counts_4 = k_modes_count(vector_4)\n",
    "    print(\"Cluster 4:\")\n",
    "    for sublist, count in sublist_counts_4.items():\n",
    "        print(f\"{sublist}: {count} times\")\n",
    "    sublist_counts_5 = k_modes_count(vector_5)\n",
    "    print(\"Cluster 5:\")\n",
    "    for sublist, count in sublist_counts_5.items():\n",
    "        print(f\"{sublist}: {count} times\")\n",
    "    sublist_counts_6 = k_modes_count(vector_6)\n",
    "    print(\"Cluster 6:\")\n",
    "    for sublist, count in sublist_counts_6.items():\n",
    "        print(f\"{sublist}: {count} times\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "    count_A_DC_1, count_A_DI_1, count_B_1 = count_three_instructions(new_labels, instructions, sorted_session_codes, cluster_1_codes)\n",
    "    count_A_DC_2, count_A_DI_2, count_B_2 = count_three_instructions(new_labels, instructions, sorted_session_codes, cluster_2_codes)\n",
    "    count_A_DC_3, count_A_DI_3, count_B_3 = count_three_instructions(new_labels, instructions, sorted_session_codes, cluster_3_codes)\n",
    "    count_A_DC_4, count_A_DI_4, count_B_4 = count_three_instructions(new_labels, instructions, sorted_session_codes, cluster_4_codes)\n",
    "    count_A_DC_5, count_A_DI_5, count_B_5 = count_three_instructions(new_labels, instructions, sorted_session_codes, cluster_5_codes)\n",
    "    count_A_DC_6, count_A_DI_6, count_B_6 = count_three_instructions(new_labels, instructions, sorted_session_codes, cluster_6_codes)\n",
    "\n",
    "    data = np.array([[count_A_DC_1, count_A_DC_2, count_A_DC_3, count_A_DC_4, count_A_DC_5, count_A_DC_6], [count_A_DI_1, count_A_DI_2, count_A_DI_3, count_A_DI_4, count_A_DI_5, count_A_DI_6], [count_B_1, count_B_2, count_B_3, count_B_4, count_B_5, count_B_6]])\n",
    "    chi2, p, _, _ = chi2_contingency(data)\n",
    "    print(f'A_DC_1: {count_A_DC_1},', f'A_DC_2: {count_A_DC_2}', f'A_DC_3: {count_A_DC_3}', f'A_DC_4: {count_A_DC_4}', f'A_DC_5: {count_A_DC_5}', f'A_DC_6: {count_A_DC_6}')\n",
    "    print(f'A_DI_1: {count_A_DI_1},', f'A_DI_2: {count_A_DI_2}', f'A_DI_3: {count_A_DI_3}', f'A_DI_4: {count_A_DI_4}', f'A_DI_5: {count_A_DI_5}', f'A_DI_6: {count_A_DI_6}')\n",
    "    print(f'B_1: {count_B_1},', f'B_2: {count_B_2}', f'B_3: {count_B_3}', f'B_4: {count_B_4}', f'B_5: {count_B_5}', f'B_6: {count_B_6}')\n",
    "    print(f\"Chi-square statistic: {chi2},\", f\"P-value: {p}\")\n",
    "    print(\"\")\n",
    "\n",
    "    data = np.array([[count_A_DC_1 + count_A_DI_1, count_B_1], [count_A_DC_2 + count_A_DI_2, count_B_2], [count_A_DC_3 + count_A_DI_3, count_B_3], [count_A_DC_4 + count_A_DI_4, count_B_4], [count_A_DC_5 + count_A_DI_5, count_B_5], [count_A_DC_6 + count_A_DI_6, count_B_6]])\n",
    "    chi2, p, _, _ = chi2_contingency(data)\n",
    "    print(f'A_1: {count_A_DC_1 + count_A_DI_1},', f'A_2: {count_A_DC_2 + count_A_DI_2}', f'A_3: {count_A_DC_3 + count_A_DI_3}', f'A_4: {count_A_DC_4 + count_A_DI_4}', f'A_5: {count_A_DC_5 + count_A_DI_5}', f'A_6: {count_A_DC_6 + count_A_DI_6}')\n",
    "    print(f'B_1: {count_B_1},', f'B_2: {count_B_2}', f'B_3: {count_B_3}', f'B_4: {count_B_4}', f'B_5: {count_B_5}', f'B_6: {count_B_6}')\n",
    "    print(f\"Chi-square statistic: {chi2},\", f\"P-value: {p}\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "    count_ja_1, count_nein_1 = count_prior_knowledge(new_labels, df_fin, sorted_session_codes, cluster_1_codes)\n",
    "    count_ja_2, count_nein_2 = count_prior_knowledge(new_labels, df_fin, sorted_session_codes, cluster_2_codes)\n",
    "    count_ja_3, count_nein_3 = count_prior_knowledge(new_labels, df_fin, sorted_session_codes, cluster_3_codes)\n",
    "    count_ja_4, count_nein_4 = count_prior_knowledge(new_labels, df_fin, sorted_session_codes, cluster_4_codes)\n",
    "    count_ja_5, count_nein_5 = count_prior_knowledge(new_labels, df_fin, sorted_session_codes, cluster_5_codes)\n",
    "    count_ja_6, count_nein_6 = count_prior_knowledge(new_labels, df_fin, sorted_session_codes, cluster_6_codes)\n",
    "\n",
    "    data = np.array([[count_ja_1, count_ja_2, count_ja_3, count_ja_4, count_ja_5, count_ja_6], [count_nein_1, count_nein_2, count_nein_3, count_nein_4, count_nein_5, count_nein_6]])\n",
    "    chi2, p, _, _ = chi2_contingency(data)\n",
    "    print(f'Ja_1: {count_ja_1},', f'Ja_2: {count_ja_2}', f'Ja_3: {count_ja_3}', f'Ja_4: {count_ja_4}', f'Ja_5: {count_ja_5}', f'Ja_6: {count_ja_6}')\n",
    "    print(f'Nein_1: {count_nein_1},', f'Nein_2: {count_nein_2}', f'Nein_3: {count_nein_3}', f'Nein_4: {count_nein_4}', f'Nein_5: {count_nein_5}', f'Nein_6: {count_nein_6}')\n",
    "    print(f\"Chi-square statistic: {chi2},\", f\"P-value: {p}\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "    bll_1 = bll_cluster(df_fin, cluster_1_codes)\n",
    "    bll_2 = bll_cluster(df_fin, cluster_2_codes)\n",
    "    bll_3 = bll_cluster(df_fin, cluster_3_codes)\n",
    "    bll_4 = bll_cluster(df_fin, cluster_4_codes)\n",
    "    bll_5 = bll_cluster(df_fin, cluster_5_codes)\n",
    "    bll_6 = bll_cluster(df_fin, cluster_6_codes)\n",
    "    print(kruskal(bll_1, bll_2, bll_3, bll_4, bll_5, bll_6))\n",
    "    print(\"bll_1:\", bll_1)\n",
    "    print(\"bll_2:\", bll_2)\n",
    "    print(\"bll_3:\", bll_3)\n",
    "    print(\"bll_4:\", bll_4)\n",
    "    print(\"bll_5:\", bll_5)\n",
    "    print(\"bll_6:\", bll_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_of_the_directory = '///' # Directory with Labeled files\n",
    "file = Path(path_of_the_directory).glob('**/*.pkl')\n",
    "\n",
    "new_labels = []\n",
    "sorted_session_codes = []\n",
    "instructions = []\n",
    "\n",
    "for i in file:\n",
    "    with open(i, 'rb') as fp:\n",
    "        separate_actions_dict = pickle.load(fp)\n",
    "\n",
    "    new_labels.append(separate_actions_dict['single_exp'])\n",
    "    sorted_session_codes.append(separate_actions_dict['session_code'])\n",
    "    instructions.append(separate_actions_dict['group'] + '_' + separate_actions_dict['subgroup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_non_opt = []\n",
    "vector_cvs = []\n",
    "vector_range_steps = []\n",
    "\n",
    "for student in new_labels:\n",
    "\n",
    "    cvs = []\n",
    "    cvs_non_cvs = []\n",
    "    for i in range(len(student['CVS'])):\n",
    "        if student['CVS'][i] in ['CVS_explore_width', 'CVS_explore_concentration', 'CVS_explore_wavelength', 'CVS_explore_solution', 'CVS_record_width', 'CVS_record_concentration', 'CVS_record_wavelength', 'CVS_record_solution']:\n",
    "            cvs.append(student['CVS'][i])\n",
    "        if student['CVS'][i] in ['CVS_explore_width', 'CVS_explore_concentration', 'CVS_explore_wavelength', 'CVS_explore_solution', 'CVS_record_width', 'CVS_record_concentration', 'CVS_record_wavelength', 'CVS_record_solution', 'Non_CVS_explore_width', 'Non_CVS_explore_concentration', 'Non_CVS_explore_wavelength', 'Non_CVS_explore_solution', 'Non_CVS_record_width', 'Non_CVS_record_concentration', 'Non_CVS_record_wavelength', 'Non_CVS_record_solution']:\n",
    "            cvs_non_cvs.append(student['CVS'][i])  \n",
    "\n",
    "    vector_cvs.append(len(cvs)/len(cvs_non_cvs))\n",
    "\n",
    "    non_opt = []\n",
    "    non_opt_opt = []\n",
    "    for i in range(len(student['Optimal'])):\n",
    "        if student['Optimal'][i] in ['Non_Optimal_explore_width', 'Non_Optimal_explore_concentration', 'Non_Optimal_explore_wavelength', 'Non_Optimal_record_width', 'Non_Optimal_record_concentration', 'Non_Optimal_record_wavelength']:\n",
    "            non_opt.append(student['Optimal'][i])\n",
    "        if student['Optimal'][i] in ['Non_Optimal_explore_width', 'Non_Optimal_explore_concentration', 'Non_Optimal_explore_wavelength', 'Optimal_explore_width', 'Optimal_explore_concentration', 'Optimal_explore_wavelength', 'Non_Optimal_record_width', 'Non_Optimal_record_concentration', 'Non_Optimal_record_wavelength', 'Optimal_record_width', 'Optimal_record_concentration', 'Optimal_record_wavelength']:\n",
    "            non_opt_opt.append(student['Optimal'][i])  \n",
    "\n",
    "    vector_non_opt.append(len(non_opt)/len(non_opt_opt))\n",
    "\n",
    "    range_steps = []\n",
    "    for i in range(len(student['CVS'])):\n",
    "        if student['CVS'][i] in ['CVS_explore_width', 'CVS_explore_concentration', 'CVS_explore_wavelength', 'Non_CVS_explore_width', 'Non_CVS_explore_concentration', 'Non_CVS_explore_wavelength']:\n",
    "            range_steps.append(student['Range: percentage, steps, stops'][i][0] * sigmoid(student['Range: percentage, steps, stops'][i][1], L, k, x0))\n",
    "    \n",
    "    vector_range_steps.append(np.mean(range_steps) / 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perc_cvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_cvs_array = np.array(vector_cvs)\n",
    "vector_cvs_reshaped = vector_cvs_array.reshape(-1, 1)\n",
    "\n",
    "distance_matrix = pairwise_distances(vector_cvs_reshaped, metric='euclidean')\n",
    "similarity_matrix_cvs = pairwise_kernels(distance_matrix, metric='rbf', gamma=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(similarity_matrix_cvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_2_cvs = [1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
    "         0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
    "         1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,\n",
    "         1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
    "         1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
    "         0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
    "         0, 0, 1, 0, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_clusters_one_vector(new_labels, df_fin, instructions, sorted_session_codes, labels_2_cvs, vector_cvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perc_non_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_non_opt_array = np.array(vector_non_opt)\n",
    "vector_non_opt_reshaped = vector_non_opt_array.reshape(-1, 1)\n",
    "\n",
    "distance_matrix = pairwise_distances(vector_non_opt_reshaped, metric='euclidean')\n",
    "similarity_matrix_non_opt = pairwise_kernels(distance_matrix, metric='rbf', gamma=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(similarity_matrix_non_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(2, 20)\n",
    "y = [] # A list of Silhouette scores for each n clusters\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(x, y)\n",
    "\n",
    "# Adding labels and a title\n",
    "plt.xlabel('N clusters')\n",
    "plt.ylabel('Silhouette score')\n",
    "\n",
    "plt.xlim(1, 20)\n",
    "\n",
    "custom_x_ticks = list(range(2, 21))\n",
    "plt.xticks(custom_x_ticks)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_2_non_opt = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
    "         0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_clusters_one_vector(new_labels, df_fin, instructions, sorted_session_codes, labels_2_non_opt, vector_non_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Range*sigmoid(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_range_steps_array = np.array(vector_range_steps)\n",
    "vector_range_steps_reshaped = vector_range_steps_array.reshape(-1, 1)\n",
    "\n",
    "distance_matrix = pairwise_distances(vector_range_steps_reshaped, metric='euclidean')\n",
    "similarity_matrix_range_steps = pairwise_kernels(distance_matrix, metric='rbf', gamma=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(similarity_matrix_range_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(2, 20)\n",
    "y = [] \n",
    "\n",
    "# Create the plot\n",
    "plt.plot(x, y)\n",
    "\n",
    "# Adding labels and a title\n",
    "plt.xlabel('N clusters')\n",
    "plt.ylabel('Silhouette score')\n",
    "\n",
    "plt.xlim(1, 20)\n",
    "\n",
    "custom_x_ticks = list(range(2, 21))\n",
    "plt.xticks(custom_x_ticks)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_2_range_steps = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
    "         0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n",
    "         0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
    "         0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
    "         0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
    "         0, 1, 1, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_clusters_one_vector(new_labels, df_fin, instructions, sorted_session_codes, labels_2_range_steps, vector_range_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CVS: 0 - High, 1 - Low\n",
    "\n",
    "Optimal: 0 - Low Non_opt, 1 - High Non_opt\n",
    "\n",
    "Range: 0 - Low, 1 - High"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_sequences = []\n",
    "\n",
    "for i in range(len(new_labels)):\n",
    "    vector = []\n",
    "\n",
    "    vector.append(labels_2_cvs[i])\n",
    "    vector.append(labels_2_non_opt[i])\n",
    "    vector.append(labels_2_range_steps[i])\n",
    "\n",
    "    current_sequences.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = pairwise_distances(current_sequences, metric='euclidean')\n",
    "similarity_matrix_multiple = pairwise_kernels(distance_matrix, metric='rbf', gamma=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_scores = []\n",
    "all_clusters = []\n",
    "\n",
    "# Specify the range of clusters you want to try\n",
    "for n_clusters in range(2, 8):\n",
    "    km = KModes(n_clusters=n_clusters, init='Huang', n_init=10, verbose=1)\n",
    "    clusters = km.fit_predict(similarity_matrix_multiple)\n",
    "    all_clusters.append(clusters)\n",
    "    \n",
    "    # Compute silhouette score and append to the list\n",
    "    silhouette_avg = silhouette_score(similarity_matrix_multiple, clusters)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "# Print or visualize the silhouette scores\n",
    "for n_clusters, score in zip(range(2, 8), silhouette_scores):\n",
    "    print(f\"Number of clusters: {n_clusters}, Silhouette Score: {score}\")\n",
    "    print(f\"Clusters: {all_clusters[n_clusters - 2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(2, 8)\n",
    "y = silhouette_scores\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(x, y)\n",
    "\n",
    "# Adding labels and a title\n",
    "plt.xlabel('N clusters')\n",
    "plt.ylabel('Silhouette score')\n",
    "\n",
    "plt.xlim(1, 8)\n",
    "\n",
    "custom_x_ticks = list(range(2, 9))\n",
    "plt.xticks(custom_x_ticks)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_modes_two_clusters(sorted_session_codes, multiple_labels_k_2_codes, current_sequences, new_labels, df_fin, instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_modes_three_clusters(sorted_session_codes, multiple_labels_k_3_codes, current_sequences, new_labels, df_fin, instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_modes_four_clusters(sorted_session_codes, multiple_labels_k_4_codes, current_sequences, new_labels, df_fin, instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_labels_k_5_codes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_modes_five_clusters(sorted_session_codes, multiple_labels_k_5_codes, current_sequences, new_labels, df_fin, instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_labels_k_6_codes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_modes_six_clusters(sorted_session_codes, multiple_labels_k_6_codes, current_sequences, new_labels, df_fin, instructions)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
